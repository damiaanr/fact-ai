# General instructions

**Before you start**: Make sure your environment aligns to the provided `environment.yml` or `requirements.txt`. To avoid problems, creating a new/clean environment in which to run this code is preferred. Furthermore, make sure that you read the warning below.

All code is run through `main.py` in command-line, without command line arguments/parameters. Note that all VAE models have been pre-trained (by us), and that all explanations for all datasets and dimensionality reduction techniques, for all levels of sparsitity, have already been learnt and provided. If you wish to re-train and re-learn, please delete all files inside the `results/deltas`, `results/vae_models` folders. Optionally, you could also delete all plot-data from the `results/measures` and `results/plots` folders.

By simply toggling straightfoward variables in the code of `main.py`, you can:

  1. Learn explanations (which might involve training VAE models, if not yet trained): set `train_algos` on line `151` to `True`. This will generate 'latent space plots' and 'performance plots' which are also stored in the `results/plots` folder. Please do not forget to either toggle the pre-loading of latent data on line `83` (as otherwise, existing latent data will be loaded) or delete all files in `results/latent_data`.
      - You can optionally skip the graphs of latent spaces and metrics (for example, when bulk-training all datasets) by setting `skip_graphs` on line `154` to `True`
      - You can chose to load pre-saved deltas, instead of re-learning these (VERY much recommended!), by setting `load_saved_deltas` on line `153` to `True`
  2. Plot explanations: set `show_explanation` on line `84` to `True` and edit the corresponding data on line `85`. This will also generate a 'metrics plot'. All plots will be stored in the `results/plots` folder.
      - You can optionally choose to only plot the 'metrics plot' without restricting to a specific K, for this, set K to `None` on line `85`.
  3. Re-plot a 'performance' plot generated in step 1. The data for these plots are stored in the `results/measures` folder. Set `show_plot` on line `41` to `True`, and specify the file name on line `44`.
  
Optionally, you could also:

  - Set the minimum variance (for PCA, SPCA, KPCA, ISO, LLE) on line `61`
  - Change the number of training trials (per lambda, per K, per algorithm, per dataset) on line `5`
  - Change the different to-attempt regularization parameters (not recommended) on line `223`
  - Change the verbose interval of the explanation-learning method on line `226`
  - Set an additional scale factor for datasets on line `94` and `164` (currently only for the Glass dataset)
  - Toggle the pre-loading of latent data on line `83` (necessary when you re-train the models and explanations)
  
Enjoy!

# Different dimensionality reduction algorithms

All dimensionality reduction algorithms are stored in `dimensionality_reduction_algorithms.py`. It is possible, but not recommended, to disable some of these dimensionality reduction algorithms by simpling 'commenting out' those of choice on lines `27-33`. Please note that the all dimensionality reduction algorithms, except VAE, are loaded through external libraries. The code for VAE is inside the `VAE` folder.

The latent spaces generated by the dimensionality reduction algorithms are stored, in combination with their generated clusters, in `results/latent_data`. Not pre-loading this latent data for existing learnt explanations and trained models could cause errors when the code is executed across different environments (since some of the phases from input data X to latent data X and latent labels Y are probabilistic). Re-training is also an option.

# Different datasets

All data is stored in the `data` folder. New lines separate samples, while tabs separate the features of a sample. As we are using K-means for clustering, we do not need more than X. It is possible, and also recommended, to only load a few of the datasets. The datasets which do not have to be loaded can be simply 'commented out' on lines `65-70` of `main.py`.

# Important warning

Currently, there is a bug in `NumPy` which causes the library to crash while using `scipy` when `matplotlib` is also used (see, among others, [here](https://stackoverflow.com/questions/64015312/why-does-adding-a-subplot-to-a-matplotlib-figure-cause-numpy-to-crash) and [here](https://stackoverflow.com/questions/63373640/svd-does-not-converge-error-when-creating-matplotlib-subplots-and-calling-np-r)). This problem could cause an *“SVD does not converge”* error, indicating a problem with our code, which is _not_ the case. For this reason, we have provided an `environment.yml` file, in addition to a simpler `requirements.txt` file. If you encounter this error, please follow the following instructions:

  1. Set `skip_graphs` on line `154` to `True`. Graphs will not be plotted anymore.
  2. Run the file normally
  3. Look into the `results/measures` folder, and find out which `.pickle`-files have been created during the last run.
  4. Plot these `.pickle` files manually by setting `show_plot` on line `41` to `True`, and specifying the file name on line `44`.